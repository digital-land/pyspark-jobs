# Deployment Documentation

This directory contains all documentation related to building, packaging, and deploying PySpark jobs to AWS EMR Serverless.

## ğŸ“š Available Guides

### ğŸ—ï¸ **Build & Packaging**
- **[Build Guide](./BUILD_GUIDE.md)**
  - Complete guide for building and deploying to AWS EMR Serverless
  - Automated build script usage (`build_aws_package.sh`)
  - S3 upload and EMR configuration

- **[Build Artifacts Guide](./BUILD_ARTIFACTS_GUIDE.md)**
  - Understanding the dual-artifact approach
  - `dist/` vs `build_output/` directories
  - Development vs production workflows

### ğŸ“Š **Data Processing & Output**
- **[Parquet to SQLite Guide](./PARQUET_TO_SQLITE_GUIDE.md)**
  - Converting parquet files to SQLite databases
  - Separate dedicated script for clean architecture
  - Local analysis, backup, and portable data files

## ğŸ¯ Quick Navigation

| Need | Start Here |
|------|------------|
| **First-time Deployment** | [Build Guide](./BUILD_GUIDE.md) |
| **Understanding Build Output** | [Build Artifacts Guide](./BUILD_ARTIFACTS_GUIDE.md) |
| **Creating SQLite Backups** | [Parquet to SQLite Guide](./PARQUET_TO_SQLITE_GUIDE.md) |

## ğŸš€ Quick Start Commands

### Build and Deploy
```bash
# Build only
./build_aws_package.sh

# Build and upload to S3
./build_aws_package.sh --upload

# Convert parquet to SQLite
./bin/convert_parquet_to_sqlite.sh \
  --input s3://bucket/parquet/ \
  --output ./output.sqlite
```

## ğŸ“¦ Deployment Architecture

```
Build Process:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Source Code â”‚ => â”‚ Python Package  â”‚ => â”‚ EMR Deployment  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ (wheel + deps)  â”‚    â”‚ (S3 artifacts)  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Artifacts Created:
â”œâ”€â”€ dist/                     # Development artifacts
â”‚   â””â”€â”€ pyspark_jobs-*.whl   # Python wheel package
â”‚
â””â”€â”€ build_output/            # Deployment artifacts
    â”œâ”€â”€ whl_pkg/             # Application wheel
    â”œâ”€â”€ dependencies/        # External dependencies
    â”œâ”€â”€ entry_script/        # Job entry points
    â””â”€â”€ upload_to_s3.sh      # Upload script
```

## ğŸ“Š File Processing Flow

```
ETL Pipeline:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw Data    â”‚ => â”‚ Main Pipeline   â”‚ => â”‚ Parquet Files   â”‚
â”‚ (CSV/JSON)  â”‚    â”‚ (Spark ETL)     â”‚    â”‚ (S3 Storage)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                     â”‚
Optional Output:                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚ SQLite Filesâ”‚ <= â”‚ SQLite Converterâ”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ (Portable)  â”‚    â”‚ (Separate Tool) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Configuration Files

| File | Purpose | Location |
|------|---------|----------|
| `build_aws_package.sh` | Main build script | Project root |
| `setup.py` | Python packaging | Project root |
| `requirements.txt` | Production deps | Project root |
| `requirements-local.txt` | Development deps | Project root |
| `deployment_manifest.json` | EMR configuration | `build_output/` |

## ğŸš¨ Common Deployment Issues

1. **Build Fails** 
   - Check Python 3.8+ and pip installation
   - Verify `setup.py` is valid
   - Ensure in project root directory

2. **Upload Fails**
   - Verify AWS CLI configuration
   - Check S3 bucket permissions
   - Confirm bucket exists and is accessible

3. **EMR Job Fails**
   - Check deployment manifest for correct S3 paths
   - Verify all dependencies are packaged
   - Review EMR Serverless logs

4. **SQLite Conversion Issues**
   - Check parquet file permissions
   - Verify Spark memory settings for large datasets
   - Use partitioned method for datasets > 500k rows

## ğŸ“ˆ Best Practices

### Development Workflow
1. **Test locally first** using `dist/` artifacts
2. **Use organized structure** from `build_output/`
3. **Verify checksums** if deployment issues occur
4. **Monitor build logs** for warnings or errors

### Production Deployment
1. **Use automated builds** via CI/CD
2. **Test in staging** environment first
3. **Monitor EMR execution** logs
4. **Backup artifacts** to version-controlled storage

---

[â† Back to Main Documentation](../README.md)
