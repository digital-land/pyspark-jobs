# Build Artifacts Guide

## Overview

The PySpark Jobs project uses a dual-artifact approach for builds to support both development and deployment workflows.

## Build Artifact Structure

### ğŸ“ **Intermediate Artifacts** (Development)
```
dist/                           # Standard Python packaging output
â”œâ”€â”€ pyspark_jobs-0.1.0-py3-none-any.whl

build/                          # Python build intermediates
â”œâ”€â”€ lib/
â””â”€â”€ bdist.macosx-*/
```

### ğŸ“ **Deployment Artifacts** (Production)
```
build_output/                   # Organized EMR deployment structure
â”œâ”€â”€ whl_pkg/
â”‚   â””â”€â”€ pyspark_jobs-0.1.0-py3-none-any.whl    # Copy from dist/
â”œâ”€â”€ dependencies/
â”‚   â””â”€â”€ dependencies.zip
â”œâ”€â”€ entry_script/
â”‚   â””â”€â”€ run_main.py
â”œâ”€â”€ jars/
â”‚   â””â”€â”€ postgresql-42.7.4.jar
â”œâ”€â”€ deployment_manifest.json
â””â”€â”€ upload_to_s3.sh
```

## Why Keep Both?

### âœ… **Development Benefits**

#### **Local Testing & Debugging**
```bash
# Test wheel directly from dist/
pip install dist/pyspark_jobs-0.1.0-py3-none-any.whl

# Inspect wheel contents
wheel unpack dist/pyspark_jobs-0.1.0-py3-none-any.whl

# Compare artifacts if issues arise
diff <(unzip -l dist/*.whl) <(unzip -l build_output/whl_pkg/*.whl)
```

#### **Incremental Development**
- **Faster rebuilds** - Setup.py can skip unchanged files
- **Tool compatibility** - IDEs and Python tools expect `dist/` 
- **Standard workflow** - Follows Python packaging conventions

### âœ… **Deployment Benefits**

#### **Organized Structure**
- **All artifacts together** - Everything needed for EMR in one place
- **Clear dependencies** - JARs, dependencies, scripts organized
- **Upload scripts** - Ready-to-deploy with manifests

#### **Production Safety**
- **Isolated artifacts** - Deployment doesn't depend on dev files
- **Version control** - Deployment manifest tracks exact versions
- **Clean separation** - Dev vs prod environments

## Build Process Flow

### 1. **Clean Previous Builds**
```bash
# Removes old artifacts
rm -rf dist/ build/ build_output/
```

### 2. **Python Package Build**
```bash
# Creates wheel in dist/
python setup.py bdist_wheel
```

### 3. **Copy to Deployment Structure**
```bash
# Organized deployment artifacts
cp dist/*.whl build_output/whl_pkg/
```

### 4. **Preserve Both**
```bash
# Keep dist/ for development
# Keep build_output/ for deployment
```

## Usage Scenarios

### ğŸ”§ **Development Workflow**

```bash
# Build and test locally
./bin/build_aws_package.sh

# Install for local testing
pip install dist/pyspark_jobs-0.1.0-py3-none-any.whl

# Test specific functionality
python -c "from jobs.utils.logger_config import setup_logging; print('âœ… Import works')"

# Inspect package contents
wheel show dist/pyspark_jobs-0.1.0-py3-none-any.whl
```

### ğŸš€ **Deployment Workflow**

```bash
# Build deployment artifacts
./bin/build_aws_package.sh

# Upload to S3
cd build_output && ./upload_to_s3.sh

# Use manifest for EMR configuration
cat build_output/deployment_manifest.json
```

### ğŸ› **Troubleshooting Workflow**

```bash
# Compare artifacts if deployment issues
ls -la dist/ build_output/whl_pkg/

# Check file integrity
md5 dist/*.whl build_output/whl_pkg/*.whl

# Test both versions
pip install dist/*.whl          # Test original
pip install build_output/whl_pkg/*.whl  # Test deployment version
```

## Git Ignore Strategy

Both directories are gitignored but serve different purposes:

```gitignore
# Development artifacts (standard Python)
build/
dist/
*.egg-info/

# Deployment artifacts (EMR-specific)
build_output/
```

### Why Both Are Ignored
- **`dist/` & `build/`** - Generated by standard Python tools
- **`build_output/`** - Generated by custom deployment script
- **Reproducible builds** - Can be regenerated from source
- **Size considerations** - Avoid large binary files in repo

## Best Practices

### âœ… **DO**
- **Keep both artifact types** for different workflows
- **Use `dist/` for development** testing and debugging
- **Use `build_output/` for deployment** to EMR
- **Verify checksums** if deployment issues occur
- **Test locally first** using `dist/` artifacts

### âŒ **DON'T**
- **Commit either to git** - Both should be regenerated
- **Mix development and deployment** - Use appropriate artifacts for each
- **Deploy from `dist/`** - Use organized `build_output/` structure
- **Skip local testing** - Always test `dist/` artifacts first

## File Size Comparison

Both artifacts are identical:
```bash
$ ls -lh dist/*.whl build_output/whl_pkg/*.whl
-rw-r--r-- 1 user staff 33K Aug 19 01:52 dist/pyspark_jobs-0.1.0-py3-none-any.whl
-rw-r--r-- 1 user staff 33K Aug 19 01:52 build_output/whl_pkg/pyspark_jobs-0.1.0-py3-none-any.whl

$ md5 dist/*.whl build_output/whl_pkg/*.whl
MD5 (dist/pyspark_jobs-0.1.0-py3-none-any.whl) = 86a0cca91d50c700472d095c04df41fe
MD5 (build_output/whl_pkg/pyspark_jobs-0.1.0-py3-none-any.whl) = 86a0cca91d50c700472d095c04df41fe
```

**Conclusion**: Keeping both provides maximum flexibility with minimal overhead! ğŸ¯

## Summary

The dual-artifact approach provides:

| Aspect | `dist/` (Development) | `build_output/` (Deployment) |
|--------|----------------------|------------------------------|
| **Purpose** | Local development & testing | EMR deployment |
| **Structure** | Standard Python packaging | Organized EMR artifacts |
| **Usage** | pip install, debugging | S3 upload, EMR jobs |
| **Tools** | Python packaging tools | Custom deployment scripts |
| **Workflow** | Development iteration | Production deployment |

**Result**: Best of both worlds - standard Python practices for development, organized deployment for production! ğŸš€
