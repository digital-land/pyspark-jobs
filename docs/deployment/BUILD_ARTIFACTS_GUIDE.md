# Build Artifacts Guide

## Overview

The PySpark Jobs project uses a dual-artifact approach for builds to support both development and deployment workflows.

## Build Artifact Structure

### 📁 **Intermediate Artifacts** (Development)
```
dist/                           # Standard Python packaging output
├── pyspark_jobs-0.1.0-py3-none-any.whl

build/                          # Python build intermediates
├── lib/
└── bdist.macosx-*/
```

### 📁 **Deployment Artifacts** (Production)
```
build_output/                   # Organized EMR deployment structure
├── whl_pkg/
│   └── pyspark_jobs-0.1.0-py3-none-any.whl    # Copy from dist/
├── dependencies/
│   └── dependencies.zip
├── entry_script/
│   └── run_main.py
├── jars/
│   └── postgresql-42.7.4.jar
├── deployment_manifest.json
└── upload_to_s3.sh
```

## Why Keep Both?

### ✅ **Development Benefits**

#### **Local Testing & Debugging**
```bash
# Test wheel directly from dist/
pip install dist/pyspark_jobs-0.1.0-py3-none-any.whl

# Inspect wheel contents
wheel unpack dist/pyspark_jobs-0.1.0-py3-none-any.whl

# Compare artifacts if issues arise
diff <(unzip -l dist/*.whl) <(unzip -l build_output/whl_pkg/*.whl)
```

#### **Incremental Development**
- **Faster rebuilds** - Setup.py can skip unchanged files
- **Tool compatibility** - IDEs and Python tools expect `dist/` 
- **Standard workflow** - Follows Python packaging conventions

### ✅ **Deployment Benefits**

#### **Organized Structure**
- **All artifacts together** - Everything needed for EMR in one place
- **Clear dependencies** - JARs, dependencies, scripts organized
- **Upload scripts** - Ready-to-deploy with manifests

#### **Production Safety**
- **Isolated artifacts** - Deployment doesn't depend on dev files
- **Version control** - Deployment manifest tracks exact versions
- **Clean separation** - Dev vs prod environments

## Build Process Flow

### 1. **Clean Previous Builds**
```bash
# Removes old artifacts
rm -rf dist/ build/ build_output/
```

### 2. **Python Package Build**
```bash
# Creates wheel in dist/
python setup.py bdist_wheel
```

### 3. **Copy to Deployment Structure**
```bash
# Organized deployment artifacts
cp dist/*.whl build_output/whl_pkg/
```

### 4. **Preserve Both**
```bash
# Keep dist/ for development
# Keep build_output/ for deployment
```

## Usage Scenarios

### 🔧 **Development Workflow**

```bash
# Build and test locally
./build_aws_package.sh

# Install for local testing
pip install dist/pyspark_jobs-0.1.0-py3-none-any.whl

# Test specific functionality
python -c "from jobs.utils.logger_config import setup_logging; print('✅ Import works')"

# Inspect package contents
wheel show dist/pyspark_jobs-0.1.0-py3-none-any.whl
```

### 🚀 **Deployment Workflow**

```bash
# Build deployment artifacts
./build_aws_package.sh

# Upload to S3
cd build_output && ./upload_to_s3.sh

# Use manifest for EMR configuration
cat build_output/deployment_manifest.json
```

### 🐛 **Troubleshooting Workflow**

```bash
# Compare artifacts if deployment issues
ls -la dist/ build_output/whl_pkg/

# Check file integrity
md5 dist/*.whl build_output/whl_pkg/*.whl

# Test both versions
pip install dist/*.whl          # Test original
pip install build_output/whl_pkg/*.whl  # Test deployment version
```

## Git Ignore Strategy

Both directories are gitignored but serve different purposes:

```gitignore
# Development artifacts (standard Python)
build/
dist/
*.egg-info/

# Deployment artifacts (EMR-specific)
build_output/
```

### Why Both Are Ignored
- **`dist/` & `build/`** - Generated by standard Python tools
- **`build_output/`** - Generated by custom deployment script
- **Reproducible builds** - Can be regenerated from source
- **Size considerations** - Avoid large binary files in repo

## Best Practices

### ✅ **DO**
- **Keep both artifact types** for different workflows
- **Use `dist/` for development** testing and debugging
- **Use `build_output/` for deployment** to EMR
- **Verify checksums** if deployment issues occur
- **Test locally first** using `dist/` artifacts

### ❌ **DON'T**
- **Commit either to git** - Both should be regenerated
- **Mix development and deployment** - Use appropriate artifacts for each
- **Deploy from `dist/`** - Use organized `build_output/` structure
- **Skip local testing** - Always test `dist/` artifacts first

## File Size Comparison

Both artifacts are identical:
```bash
$ ls -lh dist/*.whl build_output/whl_pkg/*.whl
-rw-r--r-- 1 user staff 33K Aug 19 01:52 dist/pyspark_jobs-0.1.0-py3-none-any.whl
-rw-r--r-- 1 user staff 33K Aug 19 01:52 build_output/whl_pkg/pyspark_jobs-0.1.0-py3-none-any.whl

$ md5 dist/*.whl build_output/whl_pkg/*.whl
MD5 (dist/pyspark_jobs-0.1.0-py3-none-any.whl) = 86a0cca91d50c700472d095c04df41fe
MD5 (build_output/whl_pkg/pyspark_jobs-0.1.0-py3-none-any.whl) = 86a0cca91d50c700472d095c04df41fe
```

**Conclusion**: Keeping both provides maximum flexibility with minimal overhead! 🎯

## Summary

The dual-artifact approach provides:

| Aspect | `dist/` (Development) | `build_output/` (Deployment) |
|--------|----------------------|------------------------------|
| **Purpose** | Local development & testing | EMR deployment |
| **Structure** | Standard Python packaging | Organized EMR artifacts |
| **Usage** | pip install, debugging | S3 upload, EMR jobs |
| **Tools** | Python packaging tools | Custom deployment scripts |
| **Workflow** | Development iteration | Production deployment |

**Result**: Best of both worlds - standard Python practices for development, organized deployment for production! 🚀
