name: Publish 
run-name: Publish - ${{ github.head_ref || github.ref_name }} to ${{ inputs.environment }} by @${{ github.actor }}

on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        type: environment
        description: The environment to deploy to.

jobs:
  test:
    uses: ./.github/workflows/test.yml
    secrets: inherit

  build:
    runs-on: ubuntu-latest
    needs: [test]
    outputs:
      sha_short: ${{ steps.vars.outputs.sha_short }}
      version: ${{ steps.vars.outputs.version }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.9'
      - name: Get SHA and version
        id: vars
        run: |
          echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
          echo "version=v$(date +%Y.%m.%d)-$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
      - name: Build artifacts
        run: bash build_aws_package.sh
      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: pyspark-build-artifacts
          path: build_output/
          retention-days: 7
      - name: Generate SBOM
        run: |
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
          syft . -o json > sbom.json
      - name: Upload SBOM
        uses: actions/upload-artifact@v4
        with:
          name: sbom
          path: sbom.json
          retention-days: 30

  publish-development:
    runs-on: ubuntu-latest
    needs: [build]
    if: (github.event_name == 'push' && github.ref == 'refs/heads/main') || (github.event_name == 'workflow_dispatch' && inputs.environment == 'development')
    environment: development
    env:
      AWS_S3_BUCKET: ${{ secrets.DEPLOY_PYSPARK_JOBS_CODEPACKAGE_BUCKET }}
      DOCKER_REPO: ${{ secrets.DEPLOY_DOCKER_REPOSITORY }}
    steps:
      - uses: actions/checkout@v4
      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: pyspark-build-artifacts
          path: build_output/
      - name: Install AWS CLI
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip -q awscliv2.zip
          sudo ./aws/install --update
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.DEPLOY_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.DEPLOY_AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-2
      - name: Publish to S3
        run: |
          aws s3 cp build_output/whl_pkg/ s3://$AWS_S3_BUCKET/pkg/whl_pkg/ --recursive
          aws s3 cp build_output/dependencies/dependencies.zip s3://$AWS_S3_BUCKET/pkg/dependencies/
          aws s3 cp build_output/entry_script/run_main.py s3://$AWS_S3_BUCKET/pkg/entry_script/
      - uses: aws-actions/amazon-ecr-login@v2
      - uses: docker/setup-buildx-action@v3
      - name: Build and push Docker images
        run: |
          docker buildx build \
            --platform linux/amd64,linux/arm64 \
            --tag $DOCKER_REPO:${{ needs.build.outputs.sha_short }} \
            --tag $DOCKER_REPO:latest \
            --tag $DOCKER_REPO:${{ needs.build.outputs.version }} \
            --push .

  publish-staging:
    runs-on: ubuntu-latest
    needs: [build, publish-development]
    if: (github.event_name == 'push' && github.ref == 'refs/heads/main') || (github.event_name == 'workflow_dispatch' && inputs.environment == 'staging')
    environment: staging
    env:
      AWS_S3_BUCKET: ${{ secrets.DEPLOY_PYSPARK_JOBS_CODEPACKAGE_BUCKET }}
      DOCKER_REPO: ${{ secrets.DEPLOY_DOCKER_REPOSITORY }}
    steps:
      - uses: actions/checkout@v4
      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: pyspark-build-artifacts
          path: build_output/
      - name: Install AWS CLI
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip -q awscliv2.zip
          sudo ./aws/install --update
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.DEPLOY_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.DEPLOY_AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-2
      - name: Publish to S3
        run: |
          aws s3 cp build_output/whl_pkg/ s3://$AWS_S3_BUCKET/pkg/whl_pkg/ --recursive
          aws s3 cp build_output/dependencies/dependencies.zip s3://$AWS_S3_BUCKET/pkg/dependencies/
          aws s3 cp build_output/entry_script/run_main.py s3://$AWS_S3_BUCKET/pkg/entry_script/
      - uses: aws-actions/amazon-ecr-login@v2
      - uses: docker/setup-buildx-action@v3
      - name: Build and push Docker images
        run: |
          docker buildx build \
            --platform linux/amd64,linux/arm64 \
            --tag $DOCKER_REPO:${{ needs.build.outputs.sha_short }} \
            --tag $DOCKER_REPO:latest \
            --tag $DOCKER_REPO:${{ needs.build.outputs.version }} \
            --push .

  publish-production:
    runs-on: ubuntu-latest
    needs: [build, publish-staging]
    if: (github.event_name == 'push' && github.ref == 'refs/heads/main') || (github.event_name == 'workflow_dispatch' && inputs.environment == 'production')
    environment: production
    env:
      AWS_S3_BUCKET: ${{ secrets.DEPLOY_PYSPARK_JOBS_CODEPACKAGE_BUCKET }}
      DOCKER_REPO: ${{ secrets.DEPLOY_DOCKER_REPOSITORY }}
    steps:
      - uses: actions/checkout@v4
      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: pyspark-build-artifacts
          path: build_output/
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.DEPLOY_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.DEPLOY_AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-2
      - name: Publish to S3
        run: |
          aws s3 cp build_output/whl_pkg/ s3://$AWS_S3_BUCKET/pkg/whl_pkg/ --recursive
          aws s3 cp build_output/dependencies/dependencies.zip s3://$AWS_S3_BUCKET/pkg/dependencies/
          aws s3 cp build_output/entry_script/run_main.py s3://$AWS_S3_BUCKET/pkg/entry_script/
      - uses: aws-actions/amazon-ecr-login@v2
      - uses: docker/setup-buildx-action@v3
      - name: Build and push Docker images
        run: |
          docker buildx build \
            --platform linux/amd64,linux/arm64 \
            --tag $DOCKER_REPO:${{ needs.build.outputs.sha_short }} \
            --tag $DOCKER_REPO:latest \
            --tag $DOCKER_REPO:${{ needs.build.outputs.version }} \
            --push .
